# Assessment Framework  
## Data Technologies Administration

---

## Assessment Philosophy

Assessment in this course prioritizes **architectural reasoning, governance traceability, and professional justification** over procedural tool execution.

Students are evaluated not only on whether a system works, but on:

- Why it was designed that way  
- What risk it mitigates  
- How governance is enforced  
- Whether it is defensible under audit  
- How clearly it can be explained to decision-makers  

The course moves beyond “can you run the tool?” toward  
“can you justify the architecture?”

---

# Core Assessment Principles

## 1. Governance Traceability

All major decisions must be traceable to:

- Stakeholder requirements  
- Business rules  
- Compliance obligations  
- Risk implications  

Architectural claims without traceable grounding are considered incomplete.

---

## 2. Lifecycle Integrity

Students must demonstrate understanding of:

- Raw → Staging → Core → Analytics layering  
- Controlled data movement  
- Dependency awareness  
- Lineage traceability  

Lifecycle stages must serve a governance purpose, not exist for structure alone.

---

## 3. Workload Separation

Students must justify:

- OLTP vs. OLAP separation  
- Fact vs. Dimension modeling  
- Protection against workload collision  

Architectural coherence is evaluated based on scalability and system stability reasoning.

---

## 4. Security Enforcement

Assessment evaluates the ability to:

- Apply Role-Based Access Control (RBAC)
- Enforce least-privilege principles
- Implement masking or protection strategies
- Demonstrate denial behavior as proof of control

Security must be demonstrable, not assumed.

---

## 5. Monitoring & Audit Readiness

Students must:

- Interpret audit logs
- Distinguish routine activity from suspicious behavior
- Translate log evidence into governance conclusions
- Assess whether controls failed or functioned correctly

Logs are evaluated for interpretation quality, not just retrieval.

---

## 6. Executive Communication

In the capstone defense phase, students are assessed on:

- Ability to translate technical design into business risk language
- Clarity of governance justification
- Professional tone and decision defensibility
- Acknowledgment of system limitations

Architectural maturity includes communication competence.

---

# Assessment Components

The course uses multiple complementary assessment forms.

## Structured Labs (Labs 1–6)

Labs progressively validate:

- Infrastructure readiness
- Data classification reasoning
- Layered modeling discipline
- Lifecycle execution traceability
- Access control enforcement
- Monitoring and audit interpretation

Labs provide technical evidence used later in the capstone.

---

## Semester-Long Capstone

The capstone integrates all course competencies through seven structured phases:

1. Requirements extraction
2. Logical data modeling
3. Workload strategy design
4. Governance overlay application
5. Technical enforcement validation
6. Executive architecture defense
7. Individual professional reflection

Students are assessed on coherence across phases, not isolated artifacts.

---

## Reflection

The final individual reflection assesses:

- Growth in architectural reasoning
- Awareness of governance responsibility
- Recognition of trade-offs
- Professional maturity

Reflection emphasizes synthesis and self-assessment.

---

# Evaluation Dimensions

Across assignments, work is evaluated along these dimensions:

| Dimension | Description |
|------------|-------------|
| **Accuracy** | Technical correctness and modeling integrity |
| **Traceability** | Clear linkage between requirements and design decisions |
| **Governance Alignment** | Consistency with ownership, sensitivity, retention, and risk |
| **Defensibility** | Ability to justify decisions under scrutiny |
| **Evidence Quality** | Strength of technical validation and monitoring interpretation |
| **Professional Communication** | Clarity, tone, and structured reasoning |

---

# What Strong Work Demonstrates

Strong submissions:

- Explain trade-offs clearly  
- Reference transcript evidence  
- Justify modeling decisions  
- Demonstrate enforcement behavior  
- Interpret monitoring evidence thoughtfully  
- Communicate architectural risk precisely  

---

# What Weak Work Typically Lacks

Weak submissions:

- Describe steps without reasoning  
- Repeat instructions without interpretation  
- Make unsupported architectural claims  
- Ignore governance implications  
- Focus only on tool execution  

---

# Alignment to Course Learning Outcomes

The assessment framework ensures that:

- SLO 1 is evaluated through classification and governance reasoning  
- SLO 2 is evaluated through lifecycle modeling and lineage analysis  
- SLO 3 is evaluated through infrastructure discussion and workload strategy  
- SLO 4 is evaluated through modeling and lifecycle tooling application  
- SLO 5 is evaluated through access control and monitoring enforcement  

Outcomes are reinforced across multiple phases rather than assessed in isolation.

---

# Closing Perspective

This course evaluates the development of professional data administration competence.

Students are not assessed solely on technical output.

They are assessed on their ability to:

- Design responsibly  
- Protect governed data  
- Interpret system behavior  
- Justify architectural decisions  
- Communicate risk clearly  

Assessment reflects the realities of enterprise data governance practice.
